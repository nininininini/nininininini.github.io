<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AiMath</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-10T13:15:17.164Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Li</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>git 仓库管理知识</title>
    <link href="http://yoursite.com/2020/04/10/git/"/>
    <id>http://yoursite.com/2020/04/10/git/</id>
    <published>2020-04-10T12:06:22.000Z</published>
    <updated>2020-04-10T13:15:17.164Z</updated>
    
    <content type="html"><![CDATA[<h1 id="git知识点"><a href="#git知识点" class="headerlink" title="git知识点"></a>git知识点</h1><ol><li><p>git:  版本控制工具</p></li><li><p>历史：cvs–svn   集中式的版本控制系统，需要中央服务器  最大的弊端是必须联网才能使用，速度慢，使用受限</p><p>​            git  分布式版本控制</p><p>github网站为开源项目免费提供git存储。 没有中央服务器，每个电脑都可以是一个完整的版本库，多人工作只需要把修改的推送给对方就可以</p><p>git 运行方式如图：</p><p><img src="/" alt="" class="lazyload" data-src="https://img-blog.csdn.net/20180903205936893?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1paUUhFTExPMjAxOA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p></li><li><p>工作区和暂存区</p><ul><li>git与其他版本控制系统的一个区别就是有暂存区的概念</li></ul><blockquote><p>工作区: 本地电脑目录</p><p>版本库：工作区隐藏目录.git, 不算工作区，而是git的版本库，其中最重要的就是暂存区（stage/index）, 还有git为我们主动创建的第一个分支master，以及指向master的一个指针（HEAD）</p></blockquote><ul><li>把文件往git版本库中添加的时候，分两步执行：</li></ul><blockquote><p>第一步：用 git add 把文件添加进去，实际上是把文件修改添加到暂存区</p><p>第二步： git commit 提交更改，实际上是把暂存区的所有内容提交到当前分支。即往master分支上提交更改。需要提交的文件修改通通放到暂存区， 然后一次性提交暂存区的所有修改到master。</p></blockquote></li><li><p>常用git服务器</p><p>github–全球最大的开源网站</p><p>码云–免费的，国内的</p><p>Coding–国内的</p></li><li><p>安装git     git –version查看版本库</p></li></ol><h1 id="git操作"><a href="#git操作" class="headerlink" title="git操作"></a>git操作</h1><ol><li><p>github上创建仓库，创建成功会生成一个网址</p></li><li><p><font color=red>获取项目-从服务器（ 云端）获取项目到本地</font></p></li><li><blockquote><p>git clone https地址====将仓库克隆到本地</p></blockquote></li><li><p>创建ssh密匙：为了保证安全性，只有具有密匙的管理员才可上传，其他人只可以下载</p><p>本地公匙粘贴到github账户setting下的 SSH and GPG keys</p></li><li><p><font color=red>将远端仓库添加到本地</font>：</p><blockquote><p>git remote add origin +url地址**======关联远端仓库(origin是默认的)</p></blockquote></li><li><p><font color=red>下拉项目更新本地</font>：</p><blockquote><p>git pull   — 从服务器更新代码</p></blockquote></li><li><p>本地推送到服务器： 现在本地创建版本库/仓库</p><blockquote><p>切换到本地仓库目录</p><p>git init   —初始化git 仓库，将这个目录可以变成仓库</p><p>git add 文件名 — 添加指定文件  (或者 git add .   —添加所有文件</p><p>git status  —查看当期那状态</p><p>git commit -m ‘我写的内容的原因‘   —提交文件，必须写不然不能推送   </p><p>git push 仓库url   或者 git push 项目默认名字 master  （例如：git push <a href="https://github.com/code369/git-test）" target="_blank" rel="noopener">https://github.com/code369/git-test）</a>    —推送文件到服务器</p></blockquote><p>如果出现push报错  可以用下面的语句</p><p><img src="/" alt="" class="lazyload" data-src="https://img-blog.csdnimg.cn/20191115170938967.png"></p><blockquote><p> git pull origin master –allow-unrelated-histories </p></blockquote></li><li><p>配置免密码推送</p><p>在网站上创建项目的时候有两种方式https 和ssh</p><ul><li><p>https 方式</p><blockquote><ol><li><p>cd ~ </p></li><li><p>vi .git-credentials     在里面写入  https://{username}:{password}@github.net </p></li><li><p>git config –global credential.helper store</p></li></ol></blockquote></li><li><p>ssh 方式</p><blockquote><ol><li>使用 ssh-keygen 生成公钥和私钥，直接按3个回车即可 </li><li>在  ~/.ssh/id_rsa.pub 里面的内容复制到coding上面的个人设置公钥中 </li><li>git clone <a href="mailto:git@git.coding.net">git@git.coding.net</a>:phpmonkey/hehe.git  即可</li></ol></blockquote></li></ul></li><li><p>冲突解决</p><blockquote><p>git pull      先下拉文件   （先服务器更新）</p><p>git diff       查看冲突</p><p>git push    服务器合并</p></blockquote></li><li><p>分支学习</p><blockquote><p>主分支：master，默认分支<br>新建分支： git branch 分支名<br>查看分支： git branch<br>切换分支： git checkout 分支名<br>（实际项目中，每个人都要在自己的分支上工作，最后再合并到如果要在master<br>上面合并分支，需要先切回到master（master是默认的主目录）</p><p>合并分支： git merge +分支名字<br>删除分支：git branch -d +分支名<br>（如果分支没有合并不能删除）<br>强制删除: git branch  -D +分支名字<br>（如果分支没有合并要删除可以使用）</p></blockquote></li><li><p>开发步骤</p><blockquote><p>（1）新建一个dev </p><p>（2）切换到dev进行开发 </p><p>（3）在dev添加文件并且提交文件 </p><p>（4）切换到master分支 </p><p>（5）将dev分支合并到master分支   git merge dev </p><p>（6）推送master到服务端 </p><p>（7）继续切换到dev进行开发</p></blockquote></li><li><p>git 常用操作小节</p><blockquote><p>git init    =====  建仓库， 初始化Git仓库</p><p>git add 说明.txt  =====     把文件（这里指“说明.txt”)纳入暂存区(还没有真正纳入版本控制，需要再一步确认)</p><p>git status =======查看暂存区状态</p><p>git commit  -m ‘…’  提交纳入仓库（要写原因所以要加 -m）====git commit  -m  “说明内容”</p><p>注意：第一次提交需要先提交姓名和邮箱，否则会报错</p><p>git log  ===== 查看提交日志</p><p>git  checkout  - -  ====删除文件还原：</p><p>git reset  – hard     =====版本号码（至少写五位)       回到历史版本号版本</p><p>git reflog    =======   回到删除的未来版本（过去将来时）</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;git知识点&quot;&gt;&lt;a href=&quot;#git知识点&quot; class=&quot;headerlink&quot; title=&quot;git知识点&quot;&gt;&lt;/a&gt;git知识点&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;git:  版本控制工具&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;历史：cvs–svn   集中
      
    
    </summary>
    
    
      <category term="git" scheme="http://yoursite.com/categories/git/"/>
    
    
  </entry>
  
  <entry>
    <title>数据增强方法2020</title>
    <link href="http://yoursite.com/2020/04/06/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%952020/"/>
    <id>http://yoursite.com/2020/04/06/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%952020/</id>
    <published>2020-04-06T13:01:10.000Z</published>
    <updated>2020-04-06T13:10:52.071Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/quincyliang/nlp-data-augmentation" target="_blank" rel="noopener">数据增强方法仓库</a></p><h1 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h1><p>人工扩展数据集</p><h1 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h1><p>生成数据集和真实数据之间的差异—噪声</p><h1 id="基于深度学习的数据增强方法"><a href="#基于深度学习的数据增强方法" class="headerlink" title="基于深度学习的数据增强方法"></a>基于深度学习的数据增强方法</h1><ol><li>特征空间增强</li></ol><p>神经网络可以将图像这种高维向量映射为低维向量，之前讨论的所有图像数据增强方法都应用于输入空间中的图像。现在可以在特征空间进行数据增强操作，例如：SMOTE算法，它是一种流行的增强方法，通过将k个最近的邻居合并以形成新实例来缓解类不平衡问题。</p><ol start="2"><li>对抗生成</li></ol><p>对抗攻击表明，图像表示的健壮性远不及预期的健壮性，Moosavi-Dezfooli等人充分证明了这一点。对抗生成可以改善学习的决策边界中的薄弱环节，提高模型的鲁棒性。</p><ol start="3"><li><p>基于GAN的数据增强</p><p>使用 GAN 生成模型来生成更多的数据，可用作解决类别不平衡问题的过采样技术。</p></li><li><p>神经风格转换</p><p>通过神经网络风格迁移来生成不同风格的数据，防止模型过拟合。</p></li></ol><p>nlp</p><p>在自然语言处理领域，被验证为有效的数据增强算法相对要少很多，下面我们介绍几种常见方法。</p><ul><li><p><strong>同义词词典</strong>（Thesaurus）：Zhang Xiang等人提出了Character-level Convolutional Networks for Text Classification，通过实验，他们发现可以将单词替换为它的同义词进行数据增强，这种同义词替换的方法可以在很短的时间内生成大量的数据。</p></li><li><p><strong>随机插入</strong>（Randomly Insert）：随机选择一个单词，选择它的一个同义词，插入原句子中的随机位置，举一个例子：“我爱中国” —&gt; “喜欢我爱中国”。</p></li><li><p><strong>随机交换</strong>（Randomly Swap）：随机选择一对单词，交换位置。</p></li><li><p><strong>随机删除</strong>（Randomly Delete）：随机删除句子中的单词。</p></li><li><p><strong>语法树结构替换</strong>：通过语法树结构，精准地替换单词。</p></li><li><p><strong>加噪</strong>（NoiseMix） (<a href="https://github.com/noisemix/noisemix)：类似于图像领域的加噪，NoiseMix提供9种单词级别和2种句子级别的扰动来生成更多的句子，例如：这是一本很棒的书，但是他们的运送太慢了。-&gt;这是本很棒的书，但是运送太慢了。" target="_blank" rel="noopener">https://github.com/noisemix/noisemix)：类似于图像领域的加噪，NoiseMix提供9种单词级别和2种句子级别的扰动来生成更多的句子，例如：这是一本很棒的书，但是他们的运送太慢了。-&gt;这是本很棒的书，但是运送太慢了。</a></p></li><li><p><strong>情境增强</strong>（Contextual Augmentation）：这种数据增强算法是用于文本分类任务的独立于域的数据扩充。通过用标签条件的双向语言模型预测的其他单词替换单词，可以增强监督数据集中的文本。</p></li><li><p><strong>生成对抗网络</strong>：利用生成对抗网络的方法来生成和原数据同分布的数据，来制造更多的数据。在自然语言处理领域，有很多关于生成对抗网络的工作：</p></li><li><ul><li>Generating Text via Adversarial Training</li><li>GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution</li><li>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</li></ul></li><li><p><strong>回译技术</strong>（Back Translation）：回译技术是NLP在机器翻译中经常使用的一个数据增强的方法。其本质就是快速产生一些翻译结果达到增加数据的目的。回译的方法可以增加文本数据的多样性，相比替换词来说，有时可以改变句法结构等，并保留语义信息。但是，回译的方法产生的数据严重依赖于翻译的质量。</p></li><li><p><strong>扩句-缩句-句法</strong>：先将句子压缩，得到句子的缩写，然后再扩写，通过这种方法生成的句子和原句子具有相似的结构，但是可能会带来语义信息的损失。</p></li><li><p><strong>无监督数据扩增</strong>（Unsupervised Data Augmentation）：通常的数据增强算法都是为有监督任务服务，这个方法是针对无监督学习任务进行数据增强的算法，UDA方法生成无监督数据与原始无监督数据具备分布的一致性，而以前的方法通常只是应用高斯噪声和Dropout噪声（无法保证一致性）。(<a href="https://arxiv.org/abs/1904.12848" target="_blank" rel="noopener">https://arxiv.org/abs/1904.12848</a>)</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/quincyliang/nlp-data-augmentation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据增强方法仓库&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;motivation&quot;&gt;&lt;a hre
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>HMM</title>
    <link href="http://yoursite.com/2020/04/01/HMM/"/>
    <id>http://yoursite.com/2020/04/01/HMM/</id>
    <published>2020-04-01T05:31:59.000Z</published>
    <updated>2020-04-01T05:42:19.078Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Graphical model</p><ol><li><p>Directed graphical model — HMM， MEMM   有方向可以刻画出依赖关系     可以刻画条件对的一些性质</p></li><li><p>Undirectied graphical model —CRF  LINEAR-CRF(sequence data)    &lt;—– LOG-LINEAR MODEL（static data—-logistic regression）</p><p>两者EM</p></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Graphical model&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Directed graphical model — HMM， MEMM   有方向可以刻画出依赖关系     可以刻画条件对的一些性质&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Undire
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>QA</title>
    <link href="http://yoursite.com/2020/04/01/QA/"/>
    <id>http://yoursite.com/2020/04/01/QA/</id>
    <published>2020-04-01T03:00:47.000Z</published>
    <updated>2020-04-01T03:06:15.321Z</updated>
    
    <content type="html"><![CDATA[<h1 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h1><ol><li>计算问题与库里问题的相似度，怎样表示，怎样评估相似度</li></ol><p>倒排表：有可能相似度很高的，再比较相似度</p><p>过滤掉没有用的Q</p><p>3 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;计算&quot;&gt;&lt;a href=&quot;#计算&quot; class=&quot;headerlink&quot; title=&quot;计算&quot;&gt;&lt;/a&gt;计算&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;计算问题与库里问题的相似度，怎样表示，怎样评估相似度&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;倒排表：有可能相似度很高的，再比较相似度&lt;/p
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>EM</title>
    <link href="http://yoursite.com/2020/04/01/EM/"/>
    <id>http://yoursite.com/2020/04/01/EM/</id>
    <published>2020-04-01T02:59:40.000Z</published>
    <updated>2020-04-01T14:19:31.859Z</updated>
    
    <content type="html"><![CDATA[<h1 id="latent-variable-model"><a href="#latent-variable-model" class="headerlink" title="latent variable model"></a>latent variable model</h1><blockquote><p>看不到的物体的本质</p><p>隐含信息可以生成出来</p><p>生成—VAE</p></blockquote><p>有隐的特征生成看的见得东西</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-6.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;latent-variable-model&quot;&gt;&lt;a href=&quot;#latent-variable-model&quot; class=&quot;headerlink&quot; title=&quot;latent variable model&quot;&gt;&lt;/a&gt;latent variable model&lt;/
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>文本匹配</title>
    <link href="http://yoursite.com/2020/04/01/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"/>
    <id>http://yoursite.com/2020/04/01/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/</id>
    <published>2020-04-01T01:32:52.000Z</published>
    <updated>2020-04-01T02:59:05.849Z</updated>
    
    <content type="html"><![CDATA[<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ol><li><p>搜索排序</p></li><li><p>客服机器人。很多意图，反馈信息</p><p>从库中找最相似的问题回答</p></li><li><p>机器翻译  衡量句子语义的相似性</p></li><li><p>推荐系统。画像和库中文本比对，推荐排位最高的</p></li></ol><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="string-based"><a href="#string-based" class="headerlink" title="string-based"></a>string-based</h2><p>最长公共子串-LCS</p><h2 id="edit-distance"><a href="#edit-distance" class="headerlink" title="edit distance"></a>edit distance</h2><p>把一串变另外一串的工作量</p><h2 id="Jaro-相似"><a href="#Jaro-相似" class="headerlink" title="Jaro 相似"></a>Jaro 相似</h2><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-3.jpg"></p><h2 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h2><h2 id="余弦距离-两个的角度z"><a href="#余弦距离-两个的角度z" class="headerlink" title="余弦距离  两个的角度z"></a>余弦距离  两个的角度z</h2><h2 id="逐点互信息"><a href="#逐点互信息" class="headerlink" title="逐点互信息"></a>逐点互信息</h2><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-4.jpg"></p><blockquote><p>两个的信息相关性多大</p></blockquote><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-5.jpg"></p><p>字符</p><p>利用全局的量</p><h2 id="单语义文本匹配"><a href="#单语义文本匹配" class="headerlink" title="单语义文本匹配"></a>单语义文本匹配</h2><p>NN，各个句子映射为不同的向量，句子之间没有交互</p><h2 id="多语义文档表达"><a href="#多语义文档表达" class="headerlink" title="多语义文档表达"></a>多语义文档表达</h2><p>从更多粒度衡量相似性</p><p>匹配之前大量交互—局部相似度，不会丢失局部信息</p><p>互相位置运算</p><h2 id="做文档匹配遇到的问题"><a href="#做文档匹配遇到的问题" class="headerlink" title="做文档匹配遇到的问题"></a>做文档匹配遇到的问题</h2><ol><li><p>是否是同性质的文档</p><p>是从不同的问题中找和从不同答案中找</p><p>不同的问题的话需要参数共享  问题和答案匹配的话参数不应该共享</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;应用场景&quot;&gt;&lt;a href=&quot;#应用场景&quot; class=&quot;headerlink&quot; title=&quot;应用场景&quot;&gt;&lt;/a&gt;应用场景&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;搜索排序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;客服机器人。很多意图，反馈信息&lt;/p&gt;
&lt;p&gt;从库中找最相似的
      
    
    </summary>
    
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
  </entry>
  
  <entry>
    <title>read-papers</title>
    <link href="http://yoursite.com/2020/03/31/read-papers/"/>
    <id>http://yoursite.com/2020/03/31/read-papers/</id>
    <published>2020-03-30T23:36:42.000Z</published>
    <updated>2020-04-01T01:22:55.436Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mining-and-summarizing-customer"><a href="#mining-and-summarizing-customer" class="headerlink" title="mining and summarizing customer"></a>mining and summarizing customer</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol><li>提出解决方案</li><li>比较不同</li></ol><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>前言</p><blockquote><p>对abstract的扩充</p></blockquote><ol><li>展开解决方案</li><li>详细创新点</li><li>详细各执行步骤</li></ol><h1 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h1><h1 id="method"><a href="#method" class="headerlink" title="method"></a>method</h1><ol><li><p>POS</p><p>fuzzy matching — word variants  misspelling</p></li><li><p>frequent features identification</p><p>显性特征提取，隐性不能</p></li><li><p>opinion words extration</p><p>情感与语境相关</p></li><li><p>orientation identification</p><p>基于规则</p><p>wordNet  每个词打情感标签</p></li><li><p>抽取不常见属性—规则—名词</p><p>属性词和情感词都抽取</p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="pros"><a href="#pros" class="headerlink" title="pros"></a>pros</h3><ol><li>提出了一个新任务</li><li>建立有效的系统</li></ol><h3 id="cons"><a href="#cons" class="headerlink" title="cons"></a>cons</h3><ol><li><p>只考虑显性的</p></li><li><p>人工规则</p></li><li><p>没有解决介词</p></li></ol><h1 id="sentiment-analysis-by-capsules"><a href="#sentiment-analysis-by-capsules" class="headerlink" title="sentiment analysis by capsules"></a>sentiment analysis by capsules</h1><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ol><li><p>nn依赖文本表示</p></li><li><p>语言学知识需要引入进来</p><h1 id="model"><a href="#model" class="headerlink" title="model"></a>model</h1><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-0.jpg"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-1.jpg"></p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2></li><li><p>提出了一个基于capsule的模型</p></li><li><p>不需要语言学知识可以达到很好的效果</p></li><li><p>实验显示鲁棒性</p></li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>   <img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200401-2.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;mining-and-summarizing-customer&quot;&gt;&lt;a href=&quot;#mining-and-summarizing-customer&quot; class=&quot;headerlink&quot; title=&quot;mining and summarizing custome
      
    
    </summary>
    
    
      <category term="read paper" scheme="http://yoursite.com/categories/read-paper/"/>
    
    
  </entry>
  
  <entry>
    <title>glove-elmo</title>
    <link href="http://yoursite.com/2020/03/30/glove-elmo/"/>
    <id>http://yoursite.com/2020/03/30/glove-elmo/</id>
    <published>2020-03-30T06:34:47.000Z</published>
    <updated>2020-03-30T15:01:27.017Z</updated>
    
    <content type="html"><![CDATA[<h1 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h1><h2 id="LSA"><a href="#LSA" class="headerlink" title="LSA"></a>LSA</h2><p>信息检索模型，分析文本中出现词的情况，提取出次与此之间潜在的语义结构，用这种潜在的予以结构表示词和文本（消除词之间的相关性以及简化文本向量实现降维）</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-10.jpg"></p><p>奇异值分解—降维</p><p>LSA—-LDA</p><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>LSA缺少语境 只考虑在不在同一篇文档</p><p>Word2vec缺少统计信息</p><p>既考虑语境也考虑统计信息—glove</p><h2 id="glove-wi词向量"><a href="#glove-wi词向量" class="headerlink" title="glove   wi词向量"></a>glove   wi词向量</h2><p><a href="https://blog.csdn.net/weixin_36711901/article/details/78508798" target="_blank" rel="noopener">glove理解</a></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-11.jpg"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-12.jpg"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-13.jpg"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-14.jpg"></p><h2 id="elmo"><a href="#elmo" class="headerlink" title="elmo"></a>elmo</h2><h3 id="预训练词向量需要考虑两个方面"><a href="#预训练词向量需要考虑两个方面" class="headerlink" title="预训练词向量需要考虑两个方面"></a>预训练词向量需要考虑两个方面</h3><ol><li>词的复杂特征—语法和语义</li><li>不同的文本词向量不一样</li></ol><p>语言模型：判断一句话像不像人话</p><blockquote><p>神经概率语言模型—利用神经网络来对语言模型进行建模pytorch</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;词向量&quot;&gt;&lt;a href=&quot;#词向量&quot; class=&quot;headerlink&quot; title=&quot;词向量&quot;&gt;&lt;/a&gt;词向量&lt;/h1&gt;&lt;h2 id=&quot;LSA&quot;&gt;&lt;a href=&quot;#LSA&quot; class=&quot;headerlink&quot; title=&quot;LSA&quot;&gt;&lt;/a&gt;LSA&lt;/h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>noisy-channel-model-语言模型</title>
    <link href="http://yoursite.com/2020/03/30/noisy-channel-model-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/03/30/noisy-channel-model-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-03-30T01:36:30.000Z</published>
    <updated>2020-03-30T06:34:27.408Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-2.jpg"></p><h2 id="LM-语言模型用来判断是否一句话从语法上通顺"><a href="#LM-语言模型用来判断是否一句话从语法上通顺" class="headerlink" title="LM : 语言模型用来判断是否一句话从语法上通顺"></a>LM : 语言模型用来判断是否一句话从语法上通顺</h2><p>根据模型参数可以计算出概率 </p><p>chain rule for LM</p><h3 id="Markov-assumption—-减少复杂度"><a href="#Markov-assumption—-减少复杂度" class="headerlink" title="Markov assumption—-减少复杂度"></a>Markov assumption—-减少复杂度</h3><ol><li><p>只考虑最相近的信息，基于太远的信息影响对当前信息影响不大</p></li><li><p>bigram, trigram, 4-gram</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-3.jpg"></p></li></ol><h3 id="LSTM-从前往后，不能理解一个词在前后的意思（X-Net）"><a href="#LSTM-从前往后，不能理解一个词在前后的意思（X-Net）" class="headerlink" title="LSTM 从前往后，不能理解一个词在前后的意思（X-Net）"></a>LSTM 从前往后，不能理解一个词在前后的意思（X-Net）</h3><blockquote><p><font color=red>CRF前后都考虑</font></p></blockquote><p>unigram 不考虑上下文信息，两个句子顺序不同概率一样</p><p>n-gram 会有稀疏问题——解决：分布表示（深度学习）</p><p>训练时统计的过程</p><p>思考：不放在特定的任务中验证语言模型的好坏？</p><p>评估语言模型</p><p>perplexity： 大于0的一个值</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-4.jpg"></p><p>信息混淆程度，越小越好</p><p><font color=red>为什么2次方？当前模型去离合测试集的不确定性大小？</font></p><p><font color=blue>LDA也可以计算perplexity，生成式的都可以计算perplexity</font></p><p>个别概率为0，导致整个概率为0—-需要smoothing</p><p>训练集中不存在不确定测试集中不出现—我看到的不一定是我知道所有东西。</p><h2 id="smoothing—–统计得到"><a href="#smoothing—–统计得到" class="headerlink" title="smoothing—–统计得到"></a>smoothing—–统计得到</h2><ol><li><p>Add-one smoothing—laplace smoothing</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-5.jpg"></p><p>加v保证概率和为1</p></li><li><p>Add-k smoothing</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-6.jpg"></p></li><li><p>interpolation</p><p>unigram 的信息也很重要，既要考虑bigram也要考虑unigram</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-7.jpg"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-8.jpg"></p></li><li><p>Good-turning smoothing</p><p>给定可能出现的词一定的概率，所以要变化已有词的概率</p><p>N1：出现一次的单词个数</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-9.jpg"></p></li></ol><blockquote><p>不存在的用模型预测值</p><p><font color=red>平滑可以看成是贝叶斯的特例</font></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/&quot; alt=&quot;&quot; class=&quot;lazyload&quot; data-src=&quot;/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-2.jpg&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;LM-语言模型用来判断是否一句话从语法上通顺&quot;&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>句子相似度</title>
    <link href="http://yoursite.com/2020/03/29/%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <id>http://yoursite.com/2020/03/29/%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</id>
    <published>2020-03-29T15:38:13.000Z</published>
    <updated>2020-04-01T03:00:35.305Z</updated>
    
    <content type="html"><![CDATA[<h1 id="句子相似度"><a href="#句子相似度" class="headerlink" title="句子相似度"></a>句子相似度</h1><ol><li><p>余弦相似度</p></li><li><p>欧式距离</p></li><li><p>绝对距离</p></li><li><p>minkobxki pister   p次方 1/p</p></li><li><p>mahalanlkis distance   映射到另外一个空间，转换的方法是线性的</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200329-3.jpg"></p></li><li><p>kernel distance：  kernal funtion 之后的距离</p></li></ol><p>TF-IDF</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200330-0.jpg"></p><h2 id="词之间的相似度"><a href="#词之间的相似度" class="headerlink" title="词之间的相似度"></a>词之间的相似度</h2><blockquote><ol><li><p>one-hot 是无法计算词之间的相似度—–所以用到词向量的方法（distributed representation）</p></li><li><p>使用词向量的方法表示后就可以用余弦相似度来计算两个词之间的相似度了</p></li><li><p>分布式表示（词向量）</p><p>单词—向量   句子—向量  网络—向量  代码模块—向量    <font color=blue>embedding</font></p></li><li><p>学习词向量</p><p>. 不考虑上下文        glove， skip-gram</p><p>. 考虑上下文的        context-aware—bert， elmo</p></li><li><p>有了词向量之后可以画图</p><p><font color=red>T-SNE算法  降维</font>   </p><p><font color=red>欧式空间，黎曼空间中学习</font></p></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;句子相似度&quot;&gt;&lt;a href=&quot;#句子相似度&quot; class=&quot;headerlink&quot; title=&quot;句子相似度&quot;&gt;&lt;/a&gt;句子相似度&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;余弦相似度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;欧式距离&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;绝对距离
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>拼写错误纠正</title>
    <link href="http://yoursite.com/2020/03/29/%E6%8B%BC%E5%86%99%E9%94%99%E8%AF%AF%E7%BA%A0%E6%AD%A3/"/>
    <id>http://yoursite.com/2020/03/29/%E6%8B%BC%E5%86%99%E9%94%99%E8%AF%AF%E7%BA%A0%E6%AD%A3/</id>
    <published>2020-03-29T13:51:25.000Z</published>
    <updated>2020-03-29T15:10:13.022Z</updated>
    
    <content type="html"><![CDATA[<h2 id="错误种类"><a href="#错误种类" class="headerlink" title="错误种类"></a>错误种类</h2><h3 id="错别字"><a href="#错别字" class="headerlink" title="错别字"></a>错别字</h3><ol><li><p>改过来</p></li><li><p>寻找：</p></li><li><p>基于用户的输入，候选的单词列出来（词库排序），看哪个是最好的（编辑距离最小—需要几个单位的操作； hang distance 二进制距离）</p><p>改进的方法是生成编辑距离为1，2 的字符串，过滤，返回（<font color=red>数据增强过滤-语言模型找概率最大的</font>）</p><p>怎么过滤：候选词然后用语言模型计算概率，找概率最大的。</p><p>给定字符串s，找出最有可能正确的字符串c</p><p>noisy chanel model</p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/QQ20200329-1.jpg"></p></li></ol><blockquote><p><font color=red>写错的概率和语言模型的概率，可以分别训练，然后融合模型</font></p><p>语言模型可以看成是先验概率</p></blockquote><h3 id="语法错误"><a href="#语法错误" class="headerlink" title="语法错误"></a>语法错误</h3><p>可以通过语言模型的方式解决，计算词组的概率</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;错误种类&quot;&gt;&lt;a href=&quot;#错误种类&quot; class=&quot;headerlink&quot; title=&quot;错误种类&quot;&gt;&lt;/a&gt;错误种类&lt;/h2&gt;&lt;h3 id=&quot;错别字&quot;&gt;&lt;a href=&quot;#错别字&quot; class=&quot;headerlink&quot; title=&quot;错别字&quot;&gt;&lt;/a&gt;错别
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>文本表示基础</title>
    <link href="http://yoursite.com/2020/03/29/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/03/29/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E5%9F%BA%E7%A1%80/</id>
    <published>2020-03-29T07:42:59.000Z</published>
    <updated>2020-03-29T15:10:21.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="segmentation-method"><a href="#segmentation-method" class="headerlink" title="segmentation method"></a>segmentation method</h1><h2 id="max-matching"><a href="#max-matching" class="headerlink" title="max matching"></a>max matching</h2><ol><li><p>Forward-max matching： 一种贪心算法</p><p>词典中每次匹配的单词的长度是最大的</p><blockquote><p>定义max-length, 逐渐匹配词典里的词，有匹配则划线分词</p></blockquote></li><li><p>backward-max matching： 与前向相反（90%是一样的）</p></li></ol><blockquote><p>缺点：（贪心算法）</p><ol><li>未必选择长度最大的是最优的</li><li>只是参考词典，没有考虑词的语义</li></ol></blockquote><blockquote><p>解决：设计算法，考虑语义（合成语义）</p><ol><li>不同的分割，哪种更好—-结合语言模型, 不同句子的概率    n-gram</li><li>找出所有分割方式，希望可以在多项式时间内实现这个过程</li><li>输入—-所有分割—选择其中最好的（概率最大）</li></ol></blockquote><h2 id="？-inference—viterbi-estimation"><a href="#？-inference—viterbi-estimation" class="headerlink" title="？ inference—viterbi  estimation"></a>？ inference—viterbi  estimation</h2><p>假定每个词的概率已知</p><p>给定每个词的概率，寻找最短路径</p><p>Dp是循环的方式，所有的表填满计算  -log(p(x))（<font color=blue>最大变最小，乘变加</font>）</p><p><img src="/" alt="维特比算法" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/EF4CC2D5-B3AB-4380-8E1E-59C003CA3024.png"></p><blockquote><p> viterbi是一种dp</p><p> viterbi也用来做inference：</p><p>模型已知的情况下，预测隐变量？</p></blockquote><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;segmentation-method&quot;&gt;&lt;a href=&quot;#segmentation-method&quot; class=&quot;headerlink&quot; title=&quot;segmentation method&quot;&gt;&lt;/a&gt;segmentation method&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
    
      <category term="nlp-深度之眼" scheme="http://yoursite.com/categories/nlp-%E6%B7%B1%E5%BA%A6%E4%B9%8B%E7%9C%BC/"/>
    
    
      <category term="segmentation method" scheme="http://yoursite.com/tags/segmentation-method/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="http://yoursite.com/2020/03/29/Adam/"/>
    <id>http://yoursite.com/2020/03/29/Adam/</id>
    <published>2020-03-29T03:18:34.000Z</published>
    <updated>2020-03-29T03:21:51.345Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="nlp-深度之眼" scheme="http://yoursite.com/categories/nlp-%E6%B7%B1%E5%BA%A6%E4%B9%8B%E7%9C%BC/"/>
    
    
  </entry>
  
  <entry>
    <title>paper</title>
    <link href="http://yoursite.com/2020/03/29/paper/"/>
    <id>http://yoursite.com/2020/03/29/paper/</id>
    <published>2020-03-29T03:12:55.000Z</published>
    <updated>2020-03-29T03:12:55.923Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>第一章线性规划与单纯形法</title>
    <link href="http://yoursite.com/2020/03/29/%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E4%B8%8E%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/"/>
    <id>http://yoursite.com/2020/03/29/%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E4%B8%8E%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95/</id>
    <published>2020-03-29T00:54:03.000Z</published>
    <updated>2020-03-29T03:08:25.388Z</updated>
    
    <content type="html"><![CDATA[<h1 id="化标准型"><a href="#化标准型" class="headerlink" title="化标准型"></a>化标准型</h1><p>价值系数c，工艺或技术系数a，资源限制b，决策变量约束x</p><p>建模思路：</p><ol><li><p>确定决策变量—很重要很难</p></li><li><p>写出目标函数</p></li><li><p>找出约束条件</p></li></ol><p>线性规划的标准型：</p><ol><li>目标函数最大化： 若是最小化，则零目标函数前加符号，</li><li>约束条件等式：若是小于等于加上一个松弛变量，若是大于等于减去一个剩余变量</li><li>决策变量非负： 若小于等于0，替换变量，若无约束x’-x”</li><li>资源限量非负：两边加符号</li></ol><blockquote><p>松弛变量和剩余变量在实际问题中分别表示未被充分利用和超出的资源数， 均为转化为价值和利润，所以引入模型后它们在目标函数中的系数为0</p><p>化标准型是单纯化求解的基础</p></blockquote><h2 id="图解法"><a href="#图解法" class="headerlink" title="图解法"></a>图解法</h2><p>可行域，目标函数z平移</p><p><img src="/" alt="图解法" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/%E5%9B%BE%E8%A7%A3%E6%B3%95.png"></p><h2 id="单纯形法"><a href="#单纯形法" class="headerlink" title="单纯形法"></a>单纯形法</h2><h3 id="单纯形法的原理"><a href="#单纯形法的原理" class="headerlink" title="单纯形法的原理"></a>单纯形法的原理</h3><blockquote><p>可行解 满足约束条件的所有解–可行域</p></blockquote><p><img src="/" alt="概念" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/%E9%97%AE%E9%A2%98%E8%A7%A3%E7%9A%84%E6%A6%82%E5%BF%B5.png"></p><p><img src="/" alt="可行基" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/A0CE8255-ED99-489F-9008-63B2B77DBA77.png"></p><h3 id="几点结论："><a href="#几点结论：" class="headerlink" title="几点结论："></a>几点结论：</h3><ol><li>可行域若有界则是凸集，也可能是无界域</li><li>每个基可行解对应可行域的一个顶点</li><li>可行域有有限多个顶点</li><li>如果有最优解，必在某个顶点上得到</li></ol><p><img src="/" alt="解之间的关系" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/E80FFD0A-B5A1-4851-90C2-CB5998FF81A4.png"></p><h3 id="单纯形法迭代原理"><a href="#单纯形法迭代原理" class="headerlink" title="单纯形法迭代原理"></a>单纯形法迭代原理</h3><p><img src="/" alt="基变换" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/E3583982-A80E-4D6B-9760-99015E7E6B1C.png"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/E3583982-A80E-4D6B-9760-99015E7E6B1C.png"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/E3583982-A80E-4D6B-9760-99015E7E6B1C.png"></p><p><img src="/" alt="" class="lazyload" data-src="/Users/doouni/qq%E5%9B%BE%E7%89%87/643304D9-2806-422C-BBD5-4F920F1C29F5.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;化标准型&quot;&gt;&lt;a href=&quot;#化标准型&quot; class=&quot;headerlink&quot; title=&quot;化标准型&quot;&gt;&lt;/a&gt;化标准型&lt;/h1&gt;&lt;p&gt;价值系数c，工艺或技术系数a，资源限制b，决策变量约束x&lt;/p&gt;
&lt;p&gt;建模思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确定决策
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>线性规划</title>
    <link href="http://yoursite.com/2020/03/28/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/"/>
    <id>http://yoursite.com/2020/03/28/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92/</id>
    <published>2020-03-28T14:52:23.000Z</published>
    <updated>2020-03-29T00:37:55.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义：线性约束条件下线性目标函数的极值问题（最优决策）"><a href="#定义：线性约束条件下线性目标函数的极值问题（最优决策）" class="headerlink" title="定义：线性约束条件下线性目标函数的极值问题（最优决策）"></a>定义：线性约束条件下线性目标函数的极值问题（最优决策）</h1><h1 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h1><ol><li>可行域总是一个凸集</li><li>目标函数的一个可行解（包括最优解）一定出现在可行域的一个顶点上</li></ol><blockquote><p>有限的资源和若干竞争约束下的目标函数最大或最小化的最优策略</p></blockquote><blockquote><p>线性规划可以写成标准形式</p><p>松弛型—加入变量让不等约束变为等式约束</p></blockquote><p><img src="/" alt="单纯形算法" class="lazyload" data-src="http://m.qpic.cn/psc?/V11AnIAP0yjLkE/cX4Vyn98zHAGUajflo5h*OEd6dZYRWy2HDnRhX.f2DMcLieHxFHm4XyBztK.jwv47sCWZtSikQb21e2IUgfikg!!/b&bo=oAU4BAAAAAARB6k!&rf=viewer_4"></p><blockquote><p>用约束比较强的去替换 （常数项最小？）</p><p>若目标函数没有系数为负数的项了，说明无法再继续减小目标函数，算法达到了收敛，停止转动</p></blockquote><h2 id="避免退化–bland规则"><a href="#避免退化–bland规则" class="headerlink" title="避免退化–bland规则"></a>避免退化–bland规则</h2><p>原因：替入替出变量选择不当，算法循环旋转不停止。</p><p>退化是导致单纯算法不会收敛的唯一原因</p><h3 id="bland规则：-在选择替入变量和替出变量的时候，总是选择满足条件的下标最小值"><a href="#bland规则：-在选择替入变量和替出变量的时候，总是选择满足条件的下标最小值" class="headerlink" title="bland规则： 在选择替入变量和替出变量的时候，总是选择满足条件的下标最小值"></a>bland规则： 在选择替入变量和替出变量的时候，总是选择满足条件的下标最小值</h3><p><img src="/" alt="完整步骤" class="lazyload" data-src="http://m.qpic.cn/psc?/V11AnIAP0yjLkE/cX4Vyn98zHAGUajflo5h*PpxxDMvHFYmyNlhu5VNEMas.UegDCKPY6Hz8z49ISfuZ19AKd.1hzTRNz7xOXye2g!!/b&bo=oAU4BAAAAAARB6k!&rf=viewer_4"></p><h2 id="线性规划的对偶"><a href="#线性规划的对偶" class="headerlink" title="线性规划的对偶"></a>线性规划的对偶</h2><p>. 对偶问题的最优解相等</p><p>原因：</p><ol><li>有时候对偶问题比原问题简单（原问题约束多，变量少）</li><li>单纯形算法初始基本可行解找不到</li><li>便于进行敏感度分析（如果某些已知条件发生变化，对最优解的影响程度如何）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;定义：线性约束条件下线性目标函数的极值问题（最优决策）&quot;&gt;&lt;a href=&quot;#定义：线性约束条件下线性目标函数的极值问题（最优决策）&quot; class=&quot;headerlink&quot; title=&quot;定义：线性约束条件下线性目标函数的极值问题（最优决策）&quot;&gt;&lt;/a&gt;定义：线性
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>凸优化</title>
    <link href="http://yoursite.com/2020/03/24/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2020/03/24/%E5%87%B8%E4%BC%98%E5%8C%96/</id>
    <published>2020-03-24T08:04:51.000Z</published>
    <updated>2020-03-24T08:07:54.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="几种不同的函数"><a href="#几种不同的函数" class="headerlink" title="几种不同的函数"></a>几种不同的函数</h1><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><p>1.</p><p>2.</p><h2 id="拉帕朗斯函数"><a href="#拉帕朗斯函数" class="headerlink" title="拉帕朗斯函数"></a>拉帕朗斯函数</h2><h2 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;几种不同的函数&quot;&gt;&lt;a href=&quot;#几种不同的函数&quot; class=&quot;headerlink&quot; title=&quot;几种不同的函数&quot;&gt;&lt;/a&gt;几种不同的函数&lt;/h1&gt;&lt;h2 id=&quot;凸函数&quot;&gt;&lt;a href=&quot;#凸函数&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
    
      <category term="ml" scheme="http://yoursite.com/categories/ml/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="ml" scheme="http://yoursite.com/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>study</title>
    <link href="http://yoursite.com/2020/03/24/study/"/>
    <id>http://yoursite.com/2020/03/24/study/</id>
    <published>2020-03-24T02:19:37.000Z</published>
    <updated>2020-03-24T02:49:57.624Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2020/03/24/hello-world/"/>
    <id>http://yoursite.com/2020/03/24/hello-world/</id>
    <published>2020-03-24T01:08:57.265Z</published>
    <updated>2020-03-24T07:52:46.093Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><a id="more"></a><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files-1"><a href="#Generate-static-files-1" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/server.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Server&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
